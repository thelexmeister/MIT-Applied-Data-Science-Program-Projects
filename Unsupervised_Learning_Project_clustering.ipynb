{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIOusyeT4_RJ"
   },
   "source": [
    "## Project: Unsupervised Learning\n",
    "----------------------------------------\n",
    "**Marks: 30**\n",
    "-----------------------------------------\n",
    "\n",
    "Welcome to the project on Unsupervised Learning. We will be using the Credit Card Customer Data for this project.\n",
    "\n",
    "----------------------------\n",
    "## Context: \n",
    "-----------------------------\n",
    "AllLife Bank wants to focus on its credit card customer base in the next financial year. They have been advised by their marketing research team, that the penetration in the market can be improved. Based on this input, the Marketing team proposes to run personalized campaigns to target new customers as well as upsell to existing customers. Another insight from the market research was that the customers perceive the support services of the back poorly. Based on this, the Operations team wants to upgrade the service delivery model, to ensure that customers queries are resolved faster. Head of Marketing and Head of Delivery both decide to reach out to the Data Science team for help.\n",
    "\n",
    "\n",
    "----------------------------\n",
    "## Objective: \n",
    "-----------------------------\n",
    "\n",
    "Identify different segments in the existing customer based on their spending patterns as well as past interaction with the bank.\n",
    "\n",
    "--------------------------\n",
    "## About the data:\n",
    "--------------------------\n",
    "Data is of various customers of a bank with their credit limit, the total number of credit cards the customer has, and different channels through which customer has contacted the bank for any queries, different channels include visiting the bank, online and through a call centre.\n",
    "\n",
    "- Sl_no - Customer Serial Number\n",
    "- Customer Key - Customer identification\n",
    "- Avg_Credit_Limit\t- Average credit limit (currency is not specified, you can make an assumption around this)\n",
    "- Total_Credit_Cards\t- Total number of credit cards \n",
    "- Total_visits_bank\t- Total bank visits\n",
    "- Total_visits_online -\t Total online visits\n",
    "- Total_calls_made - Total calls made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ReVFSgtvvO6"
   },
   "source": [
    "## Importing libraries and overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KJKn2dkPKnkU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn-extra in c:\\users\\lexig\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\lexig\\anaconda3\\lib\\site-packages (from scikit-learn-extra) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\lexig\\anaconda3\\lib\\site-packages (from scikit-learn-extra) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\lexig\\anaconda3\\lib\\site-packages (from scikit-learn-extra) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lexig\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\lexig\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "#Import all the necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#to scale the data using z-score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#importing clustering algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "#if the below line of code gives an error, then uncomment the following code to install the sklearn_extra library\n",
    "!pip install scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qEgcn2XvvO8"
   },
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "p4QVUUO3VHHH",
    "outputId": "6bd7504e-4281-41ca-f508-5a61927e94b1"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Credit Card Customer Data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0689e7fb33fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Credit Card Customer Data.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1072\u001b[0m                     \u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(path, content, storage_options)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mcontent_or_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     ) as handle:\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Credit Card Customer Data.xlsx'"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('Credit+Card+Customer Data.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmxlhPKovvO-"
   },
   "source": [
    "#### Check the info of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtroAFl9vvO-",
    "outputId": "bff20f83-1d85-488e-e1dc-e73c56f563a6"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPP73jY5vvPA"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "- There are 660 observations and 7 columns in the dataset.\n",
    "- All columns have 660 non-null values i.e. there are no missing values.\n",
    "- All columns are of int64 data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOP-MnWovvPA"
   },
   "source": [
    "**There are no missing values. Let us now figure out the uniques in each column.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCRC8zfDvvPB",
    "outputId": "ad647d25-ef4e-48fa-9f7e-6f1916668f0e"
   },
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjjUYpvovvPC"
   },
   "source": [
    "- Customer key, which is an identifier, has repeated values. We should treat the same accordingly before applying any algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpZolGOdvvPC"
   },
   "source": [
    "## Data Preprocessing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6bb-Fs0vvPD"
   },
   "source": [
    "#### **Question 1: Identify and drop the rows with duplicated customer keys (2 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWj8abFqvvPD"
   },
   "outputs": [],
   "source": [
    "# Identify the duplicated customer keys - There should be 5\n",
    "duplicate_keys = pd.concat(g for _, g in data.groupby(\"Customer Key\") if len(g) > 1)\n",
    "duplicate_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeUihO0KvvPF"
   },
   "outputs": [],
   "source": [
    "# Drop duplicated keys\n",
    "\n",
    "data = data.drop_duplicates(subset=['Customer Key'])\n",
    "data.nunique() # just to check that there were 5 dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDaRoTAQvvPF"
   },
   "source": [
    "We have done some basic checks. Now, let's drop the variables that are not required for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "albq4GJ2vvPF"
   },
   "outputs": [],
   "source": [
    "data.drop(columns = ['Sl_No', 'Customer Key'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRqvXDr4vvPG"
   },
   "source": [
    "Now that we have dropped unnecessary column. We can again check for duplicates. Duplicates would mean customers with identical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2EV_kXUvvPG",
    "outputId": "c4159aa4-6b34-4677-b600-063d6e13c529"
   },
   "outputs": [],
   "source": [
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7Xt_Ss3vvPG"
   },
   "source": [
    "We can drop these duplicated rows from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBnoG0IivvPH"
   },
   "outputs": [],
   "source": [
    "data=data[~data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgj7fBervvPH",
    "outputId": "7181e588-68b1-4184-ed6c-687d1997dc47"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtAIAYr9vvPH"
   },
   "source": [
    "- After removing duplicated keys and rows and unnecessary columns, there are 644 unique observations and 5 columns in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmykagZ0vvPI"
   },
   "source": [
    "#### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPGo3N4vvPI"
   },
   "source": [
    "#### **Question 2: Write your observations on the summary statistics of the data (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg-YXbPNvvPI",
    "outputId": "4c9d30bf-f6d2-4922-d340-7db9b8f6ce24"
   },
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAEzS8-GvvPI"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "Credit Card limit: Average is 34,543 +/- $37,428.70 dollars (Data is not normally distributed). The median is 18,000 dollars (IQR = 11,000 to 448,000). The range is 3,000 to 200,000 dollars.\n",
    "\n",
    "Total Number of Credit Cards each Customer has: Average is 4.69 +/- 2.18 cards. The median is 5 (IQR = 3 - 6). The range is 1 to 10.\n",
    "\n",
    "Total Bank Visits: Average is 2.40 +/- 1.63. The median is 2 (IQR = 1 - 4). The range is 0 to 5.\n",
    "\n",
    "Total Online Visits: Average is 2.63 +/- 2.96. The median is 2 (IQR = 1 - 4). The range is 0 to 15.\n",
    "\n",
    "Total Calls Made Through the Call Center: Average is 3.61 +/- 2.88. The median is 3 (IQR = 1 - 5.25). The range is 0 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUxHbmglvvPJ"
   },
   "source": [
    "#### Now let's go ahead with the exploring each variable at hand. We will check the distribution and outliers for each variable in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM81boKxvvPJ"
   },
   "source": [
    "#### Question 3:\n",
    "- **Check the distribution of all variables (use .hist() attribute) (2 Marks)**\n",
    "- **Check outliers for all variables (use sns.boxplot()) (2 Mark)**\n",
    "- **Write your observations (1 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmKqRM1JvvPJ"
   },
   "outputs": [],
   "source": [
    "# Uncomment and complete the code by filling the blanks \n",
    "\n",
    "for col in data.columns:\n",
    "     print(col)\n",
    "     print('Skew :',round(data[col].skew(),2))\n",
    "     plt.figure(figsize=(15,4))\n",
    "     plt.subplot(1,2,1)\n",
    "     data[col].hist()\n",
    "     plt.ylabel('count')\n",
    "     plt.subplot(1,2,2)\n",
    "     sns.boxplot(x=data[col])\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuraaJ3wvvPK"
   },
   "source": [
    "**Observation:**\n",
    "Avg_Credit_Limit: Skew : 2.19 (Highly Right Skewed)\n",
    "\n",
    "- Several outliers above 100,000 dollar credit limit (over 35 hard to count)\n",
    "\n",
    "Total_Credit_Cards: Skew : 0.17 (Relatively symmetric)\n",
    "\n",
    "- no outliers\n",
    "\n",
    "Total_visits_bank: Skew : 0.15 (Relatively symmetric)\n",
    "\n",
    "- no outliers\n",
    "\n",
    "Total_visits_online: Skew : 2.21 (Highly Right Skewed)\n",
    "\n",
    "- 7 outliers above 8 online visits\n",
    "\n",
    "Total_calls_made: Skew : 0.65 (Slightly Right Skewed)\n",
    "\n",
    "- no outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGeGbF8ovvPK"
   },
   "source": [
    "**Now, let's check the correlation among different variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-INtorZDvvPK",
    "outputId": "d00ff8b6-82f7-4366-8da6-f61d0e6aabe4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(data.corr(), annot=True, fmt='0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_rQynXTvvPL"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "- Avg_Credit_Limit is positively correlated with Total_Credit_Cards Total_visits_online which can makes sense.\n",
    "- Avg_Credit_Limit is negatively correlated with Total_calls_made and Total_visits_bank.\n",
    "- Total_visits_bank, Total_visits_online, Total_calls_made are negatively correlated which implies that majority of customers use only one of these channels to contact the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFimA8PUvvPL"
   },
   "source": [
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjqSX-50vvPL"
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "data_scaled=pd.DataFrame(scaler.fit_transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2cyzdZNvvPM",
    "outputId": "42074a5e-e393-4450-9f1f-75d9be79f194"
   },
   "outputs": [],
   "source": [
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ESvHP6SvvPM"
   },
   "outputs": [],
   "source": [
    "#Creating copy of the data to store labels from each algorithm\n",
    "data_scaled_copy = data_scaled.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lVGJ-HRvvPN"
   },
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCT1SB7_vvPN"
   },
   "source": [
    "Let us now fit k-means algorithm on our scaled data and find out the optimum number of clusters to use.\n",
    "\n",
    "We will do this in 3 steps:\n",
    "1. Initialize a dictionary to store the SSE for each k\n",
    "2. Run for a range of Ks and store SSE for each run\n",
    "3. Plot the SSE vs K and find the elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkQWkpSovvPN",
    "outputId": "7819694e-9ce6-4ba2-f0bd-8ccd411cb018"
   },
   "outputs": [],
   "source": [
    "# step 1\n",
    "sse = {} \n",
    "\n",
    "# step 2 - iterate for a range of Ks and fit the scaled data to the algorithm. Use inertia attribute from the clustering object and \n",
    "# store the inertia value for that k \n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000, random_state=1).fit(data_scaled)\n",
    "    sse[k] = kmeans.inertia_\n",
    "\n",
    "# step 3\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()), 'bx-')\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYIbRxO2vvPO"
   },
   "source": [
    "- Looking at the plot, we can say that elbow point is achieved for k=3.\n",
    "- We will fit the k-means again with k=3 to get the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYdAYWNNvvPO"
   },
   "source": [
    "#### Question 4: \n",
    "\n",
    "- **From the above elbow plot, state the reason for choosing k=3 and with random_state=1(1 Mark)**\n",
    "- **Fit the K-means algorithms on the scaled data with number of cluster equal to 3 (2 Mark)**\n",
    "- **Store the predictions as 'Labels' to the 'data_scaled_copy' and 'data' dataframes (2 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkeg6XsBvvPO"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(init=\"random\",n_clusters = 3,random_state = 1) #Apply the K-Means algorithm\n",
    "kmeans.fit(data_scaled) #Fit the kmeans function on the scaled data\n",
    "\n",
    "#Adding predicted labels to the original data and scaled data \n",
    "data_scaled_copy['Labels'] = kmeans.predict(data_scaled)#Save the predictions on the scaled data from K-Means\n",
    "data['Labels'] = kmeans.predict(data_scaled) #Save the predictions on the scaled data from K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SPMPRF1vvPP"
   },
   "source": [
    "We have generated the labels with k-means. Let us look at the various features based on the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuLEkmr1vvPQ"
   },
   "source": [
    "#### **Question 5: Create cluster profiles using the below summary statistics and box plots for each label (6 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDqfjsfl2lew",
    "outputId": "a60da3e1-8312-4b0c-a0ab-54b41b8d86a1"
   },
   "outputs": [],
   "source": [
    "#Number of observations in each cluster\n",
    "data.Labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu7EO0nn2lex",
    "outputId": "0716f626-abf8-435a-878d-f2bd73409fd9"
   },
   "outputs": [],
   "source": [
    "#Calculating summary statistics of the original data for each label\n",
    "mean = data.groupby('Labels').mean()\n",
    "median = data.groupby('Labels').median()\n",
    "df_kmeans = pd.concat([mean, median], axis=0)\n",
    "df_kmeans.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']\n",
    "df_kmeans.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTnGIXvWvvPQ",
    "outputId": "fb854446-d8c0-4254-8b7c-d82ca540226b"
   },
   "outputs": [],
   "source": [
    "#Visualizing different features w.r.t K-means labels\n",
    "data_scaled_copy.boxplot(by = 'Labels', layout = (1,5),figsize=(20,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nadx5OyGvvPR"
   },
   "source": [
    "**Cluster Profiles:**\n",
    "All relationships are explained relative to the median of each group.\n",
    "\n",
    "GROUP 0: Fewer cards with low credit limits (one outlier - high), and many more calls made to the bank compared to the other two groups, few visits compared to group 1 and same as group 2 and few online visits, similar to group 1 but fewer than group 2, with one outlier. Group 0 was the only group with an outlier, although it is unknown if this is the same person.\n",
    "\n",
    "GROUP 1: This group was in the middle of the three for credit limit and total cards. It had the most total bank visits of the three groups, but made a similar number of calls as group 2 and a simmilar number of visits online as group 0, but the lowest of the three.\n",
    "\n",
    "GROUP 2: This group has the highest credit limit (relatively large IQR) and the most number of cards of the three groups. It makes the most online visits (relatively large IQR), but the fewest visits and calls to the bank of the three groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B02j8pGUvvPS"
   },
   "source": [
    "## Gaussian Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYBGfwaVvvPS"
   },
   "source": [
    "Let's create clusters using Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0N7Ss06vvPS"
   },
   "source": [
    "#### Question 6: \n",
    "\n",
    "- **Apply the Gaussian Mixture algorithm on the scaled data with random_state=1 (2 Marks)** \n",
    "- **Create cluster profiles using the below summary statistics and box plots for each label (2 Marks)**\n",
    "- **Compare the clusters from both algorithms - K-means and Gaussian Mixture (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBHzul_PvvPS"
   },
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=3, random_state=1) #Apply the Gaussian Mixture algorithm\n",
    "gmm.fit(data_scaled) #Fit the gmm function on the scaled data\n",
    "\n",
    "data_scaled_copy['GmmLabels'] = gmm.predict(data_scaled)\n",
    "data['GmmLabels'] = gmm.predict(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBtCZOQB2ley",
    "outputId": "aa7194db-946b-48d4-9717-4eca8dff43eb"
   },
   "outputs": [],
   "source": [
    "#Number of observations in each cluster\n",
    "data.GmmLabels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m21wTqIgvvPT",
    "outputId": "ae29c6fb-918e-42cb-a6d8-4555f3e69df4"
   },
   "outputs": [],
   "source": [
    "#Calculating summary statistics of the original data for each label\n",
    "original_features = [\"Avg_Credit_Limit\",\"Total_Credit_Cards\",\"Total_visits_bank\",\"Total_visits_online\",\"Total_calls_made\"]\n",
    "\n",
    "mean = data.groupby('GmmLabels').mean()\n",
    "median = data.groupby('GmmLabels').median()\n",
    "df_gmm = pd.concat([mean, median], axis=0)\n",
    "df_gmm.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']\n",
    "df_gmm[original_features].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuJf3MeOvvPT",
    "outputId": "7dee619e-e69b-4586-82ab-f6635fda21a0"
   },
   "outputs": [],
   "source": [
    "# plotting boxplots with the new GMM based labels\n",
    "\n",
    "features_with_lables = [\"Avg_Credit_Limit\",\"Total_Credit_Cards\",\"Total_visits_bank\",\"Total_visits_online\",\"Total_calls_made\",\"GmmLabels\"]\n",
    "\n",
    "data_scaled_copy[features_with_lables].boxplot(by = 'GmmLabels', layout = (1,5),figsize=(20,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHA5nl2yvvPU"
   },
   "source": [
    "**Cluster Profiles:**\n",
    "All relationships are explained relative to the median of each group.\n",
    "\n",
    "GROUP 0: Lowest credit limit (one high outlier), fewest number of cards. Highest number of calls made. Visits to the bank similar to group 2 and visits online most similar to group 1 (one high outlier). The only group with an outlier.\n",
    "\n",
    "GROUP 1: Between group 0 and group 2 for the number of cards and credit limit (although most similar to group 0 for number of cards). Highest number of visits to the bank, but total calls were low (similar to group 2) and total visits online were the lowest of the three groups (similar to group 0).\n",
    "\n",
    "GROUP 2: Highest credit limit (relatively large IQR) and number of cards. Fewest calls made and visits to the bank, with the most visits online (relatively large IQR).\n",
    "\n",
    "**Comparing Clusters:**\n",
    "\n",
    "The two methods created the same stories/descriptions of the characteristics of the three groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixzPPJG6vvPV"
   },
   "source": [
    "## K-Medoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpPxOxvmvvPW"
   },
   "source": [
    "#### Question 7: \n",
    "\n",
    "- **Apply the K-Mediods on the scaled data with random_state=1 (2 Marks)** \n",
    "- **Create cluster profiles using the below summary statistics and box plots for each label (2 Marks)**\n",
    "- **Compare the clusters from both algorithms - K-Means and K-Medoids (2 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxuTu1JF48hA"
   },
   "outputs": [],
   "source": [
    "kmedo = KMedoids(metric=\"euclidean\", n_clusters=3)#Apply the K-Medoids algorithm\n",
    "kmedo.fit(data_scaled) #Fit the kmedo function on the scaled data\n",
    "\n",
    "data_scaled_copy['kmedoLabels'] = kmedo.predict(data_scaled)\n",
    "data['kmedoLabels'] = kmedo.predict(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t6hVMWs2lez",
    "outputId": "bd84b129-be61-45e3-b8e0-8428caa91e78"
   },
   "outputs": [],
   "source": [
    "#Number of observations in each cluster\n",
    "data.kmedoLabels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvKJ9rrzvvPW",
    "outputId": "c8f9fc4d-86c7-4d33-d2de-aaaba2f84d0f"
   },
   "outputs": [],
   "source": [
    "#Calculating summary statistics of the original data for each label\n",
    "mean = data.groupby('kmedoLabels').mean()\n",
    "median = data.groupby('kmedoLabels').median()\n",
    "df_kmedoids = pd.concat([mean, median], axis=0)\n",
    "df_kmedoids.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']\n",
    "df_kmedoids[original_features].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWdxV2rLvvPX",
    "outputId": "c93561c9-22a3-4181-a2c3-b2b473099d25"
   },
   "outputs": [],
   "source": [
    "#plotting boxplots with the new K-Medoids based labels\n",
    "\n",
    "features_with_lables = [\"Avg_Credit_Limit\",\t\"Total_Credit_Cards\",\"Total_visits_bank\",\"Total_visits_online\",\"Total_calls_made\",\"kmedoLabels\"]\n",
    "\n",
    "data_scaled_copy[features_with_lables].boxplot(by = 'kmedoLabels', layout = (1,5),figsize=(20,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_MtSZkr2le0"
   },
   "source": [
    "Let's compare the clusters from K-Means and K-Medoids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1gebsJ_2le0",
    "outputId": "288db421-b425-4f7f-ea68-7ab9d067250c"
   },
   "outputs": [],
   "source": [
    "comparison = pd.concat([df_kmedoids, df_kmeans], axis=1)[original_features]\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuNbMOGKvvPY"
   },
   "source": [
    "**Cluster Profiles:**\n",
    "\n",
    "All relationships are explained relative to the median of each group.\n",
    "\n",
    "GROUP 0: Lowest average credit limit (one higher outlier), fewest number of credit cards. Highest number of calls, lowest number of bank visits, highest median of visits online (one high outlier).\n",
    "\n",
    "GROUP 1: Highest credit limit (largest IQR) and most cards (one low outlier). Low number of calls (simiar to group 2), low number of bank visits (just more than group 0)(one high outlier), in the middle of the three groups for visits online (with a large IQR).\n",
    "\n",
    "GROUP 2: In the middle of the three for credit limit and number of cards. Low number of calls to the bank, similar to group 1, highest visits to the bank, but lowest online visits.\n",
    "\n",
    "**Comparing Clusters:**\n",
    "\n",
    "GROUP 0: The two clustering techniques, KMediods and KMeans, described this group almost identically with respect to means and exactly the same with respect to medians. And they each found the same outliers. I conclude that these two methods described GROUP 0 using the same people.\n",
    "\n",
    "GROUP 1 and GROUP 2: This is where the differences are between the two methods.\n",
    "\n",
    "GROUP 1: Kmediods had a higher mean and median for Average Credit Limit, Credit Cards, and Online Visits than KMeans. While for Bank Visits KMediods mean and median were lower than KMeans for this group. The two techniques reported the same mean and median for this group in the total Calls Made to the bank.\n",
    "\n",
    "GROUP 2: KMediods returned a higher mean and median for Visits to the Bank and Calls Made than KMeans. KMediods returned lower mean and median for Average Credit Limit, Number of Credit Cards, and Visits Online than KMeans.\n",
    "\n",
    "KMediods and KMeans found different members for GROUPS 1 and 2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Learner_Notebook_Unsupervised_Learning_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
